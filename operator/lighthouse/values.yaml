# Default values for lighthouse.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Pod Security Context
# ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
securityContext:
  fsGroup: 1001
  runAsUser: 1001

# Beacon chain is responsible for running a full Proof-of-Stake blockchain
beacon:
  # Whether to deploy beacon chain or not.
  enabled: true

  # How many beacon chain pods to run simultaneously
  replicas: 1

  # Docker image
  image:
    repository: "sigp/lighthouse"
    tag: "v2.0.0"
    pullPolicy: IfNotPresent

  # Enable pod disruption budget
  # https://kubernetes.io/docs/tasks/run-application/configure-pdb
  podDisruptionBudget:
    enabled: true
    maxUnavailable: 1

  # Vertical Pod Autoscaler config
  # Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler
  verticalAutoscaler:
    # If true a VPA object will be created for the StatefulSet
    enabled: false
    updateMode: Off
    containerPolicies: { }

  extraFlags:
    - "--network=mainnet"

  # The path to the beacon chain data directory. 
  dataDirPath: /data/lighthouse/beacon

  # Ethereum 1 node endpoints.
  eth1Endpoints:
    - http://127.0.0.1:8545
    - http://127.0.0.1:8546

  metrics:
    # Whether to enable metrics collection or not
    enabled: false

    port: 5054

    # Extra flags to pass for collecting metrics
    flags:
      - "--metrics"
      - "--metrics-address=0.0.0.0"

    # Metrics collection annotations to add to service
    svcAnnotations:
      prometheus.io/scrape: "true"
    
    ## Prometheus Service Monitor
    ## ref: https://github.com/coreos/prometheus-operator
    ##      https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor resource(s) for scraping metrics using PrometheusOperator
      ##
      enabled: false
      ## @param metrics.serviceMonitor.namespace The namespace in which the ServiceMonitor will be created
      ##
      namespace: ""
      ## @param metrics.serviceMonitor.interval The interval at which metrics should be scraped
      ##
      interval: 30s
      ## @param metrics.serviceMonitor.scrapeTimeout The timeout after which the scrape is ended
      ##
      scrapeTimeout: ""
      ## @param metrics.serviceMonitor.relabellings Metrics RelabelConfigs to apply to samples before scraping.
      ##
      relabellings: []
      ## @param metrics.serviceMonitor.metricRelabelings Metrics RelabelConfigs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param metrics.serviceMonitor.additionalLabels Additional labels that can be used so ServiceMonitor resource(s) can be discovered by Prometheus
      ##
      additionalLabels: {}
    ## Custom PrometheusRule to be defined
    ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions
    ##
    prometheusRule:
      ## @param metrics.prometheusRule.enabled Create a custom prometheusRule Resource for scraping metrics using PrometheusOperator
      ##
      enabled: false
      ## @param metrics.prometheusRule.namespace The namespace in which the prometheusRule will be created
      ##
      namespace: ""
      ## @param metrics.prometheusRule.additionalLabels Additional labels for the prometheusRule
      ##
      additionalLabels: {}
      ## @param metrics.prometheusRule.rules Custom Prometheus rules
      ## e.g:
      ## rules:
      ##   - alert: GethNodeDown
      ##     expr: up{job="{{ include "openethereum.fullname" . }}-node"} == 0
      ##     for: 5m
      ##     labels:
      ##       severity: error
      ##     annotations:
      ##       summary: Geth Node {{ "{{ $labels.instance }}" }} down
      ##       description: Geth Node {{ "{{ $labels.instance }}" }} is down
      ##
      rules: []

  # defines whether the service must be headless
  svcHeadless: false

  # Configure session affinity for validator clients to hit the same beacon node
  # for the period specified in `timeoutSeconds`
  # https://kubernetes.io/docs/concepts/services-networking/service/#proxy-mode-userspace
  sessionAffinity:
    # Whether to enable session affinity or not
    enabled: false
    # The session duration in seconds
    timeoutSeconds: 86400

  # Configure resource requests and limits.
  # http://kubernetes.io/docs/user-guide/compute-resources/
  resources: { }

  # Configure liveness and readiness probes
  # https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  readinessProbe:
    initialDelaySeconds: 180
    timeoutSeconds: 1
    periodSeconds: 60
    failureThreshold: 3
    successThreshold: 1
    httpGet:
      path: /lighthouse/health
      port: 5052
      scheme: HTTP
  livenessProbe:
    initialDelaySeconds: 60
    timeoutSeconds: 1
    periodSeconds: 60
    failureThreshold: 60
    successThreshold: 1
    httpGet:
      path: /lighthouse/health
      port: 5052
      scheme: HTTP

  # If false, data ownership will not be reset at startup
  # This allows the beacon node to be run with an arbitrary user
  initChownData:
    enabled: true
    # initChownData container image
    image:
      repository: "busybox"
      tag: "1.33"
      pullPolicy: IfNotPresent

  # Whether or not to allocate persistent volume disk for the data directory.
  # In case of pod failure, the pod data directory will still persist.
  persistence:
    enabled: true
    storageClassName: ""
    accessModes:
      - ReadWriteOnce
    size: 350Gi

  ## @param master.nodeSelector Node labels for Redis&trade; master pods assignment
  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  
  # Affinity Settings
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  affinity: { }

  # Toleration Settings
  # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations: [ ]

  # used to assign priority to pods
  # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  priorityClassName: ""

# Validators is responsible for performing block proposals and attestations
validator:
  # Whether to deploy beacon chain or not.
  enabled: false

  # How many beacon chain pods to run simultaneously
  replicas: 1

  # Docker image
  image:
    repository: "sigp/lighthouse"
    tag: "v2.0.0"
    pullPolicy: IfNotPresent

  # Enable pod disruption budget
  # https://kubernetes.io/docs/tasks/run-application/configure-pdb
  podDisruptionBudget:
    enabled: true
    maxUnavailable: 1

  # Vertical Pod Autoscaler config
  # Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler
  verticalAutoscaler:
    # If true a VPA object will be created for the StatefulSet
    enabled: false
    updateMode: Off
    containerPolicies: { }
  
  # This field determines how long it will wait before the validator client starts or restarts.
  # It's a way to protect the validator client from double attesting or block proposing when the DB is out-of-date.
  startWaitTime: 780

  extraFlags:
    - "--graffiti=StakeWise"
  
  networkID: "mainnet"

  # Validator keys
  secrets:
    keystore:
      data: ""
      password: ""
      publicKey: ""

  metrics:
    # Whether to enable metrics collection or not
    enabled: false

    port: 5064

    # Extra flags to pass for collecting metrics
    flags:
      - "--metrics"
      - "--metrics-address=0.0.0.0"

    # Metrics collection annotations to add to service
    svcAnnotations:
      prometheus.io/scrape: "true"
    
    ## Prometheus Service Monitor
    ## ref: https://github.com/coreos/prometheus-operator
    ##      https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor resource(s) for scraping metrics using PrometheusOperator
      ##
      enabled: false
      ## @param metrics.serviceMonitor.namespace The namespace in which the ServiceMonitor will be created
      ##
      namespace: ""
      ## @param metrics.serviceMonitor.interval The interval at which metrics should be scraped
      ##
      interval: 30s
      ## @param metrics.serviceMonitor.scrapeTimeout The timeout after which the scrape is ended
      ##
      scrapeTimeout: ""
      ## @param metrics.serviceMonitor.relabellings Metrics RelabelConfigs to apply to samples before scraping.
      ##
      relabellings: []
      ## @param metrics.serviceMonitor.metricRelabelings Metrics RelabelConfigs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param metrics.serviceMonitor.additionalLabels Additional labels that can be used so ServiceMonitor resource(s) can be discovered by Prometheus
      ##
      additionalLabels: {}
    ## Custom PrometheusRule to be defined
    ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions
    ##
    prometheusRule:
      ## @param metrics.prometheusRule.enabled Create a custom prometheusRule Resource for scraping metrics using PrometheusOperator
      ##
      enabled: false
      ## @param metrics.prometheusRule.namespace The namespace in which the prometheusRule will be created
      ##
      namespace: ""
      ## @param metrics.prometheusRule.additionalLabels Additional labels for the prometheusRule
      ##
      additionalLabels: {}
      ## @param metrics.prometheusRule.rules Custom Prometheus rules
      ## e.g:
      ## rules:
      ##   - alert: GethNodeDown
      ##     expr: up{job="{{ include "openethereum.fullname" . }}-node"} == 0
      ##     for: 5m
      ##     labels:
      ##       severity: error
      ##     annotations:
      ##       summary: Geth Node {{ "{{ $labels.instance }}" }} down
      ##       description: Geth Node {{ "{{ $labels.instance }}" }} is down
      ##
      rules: []

  # Configure resource requests and limits.
  # http://kubernetes.io/docs/user-guide/compute-resources/
  resources: { }

  # Configure liveness and readiness probes
  # https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  readinessProbe:
    initialDelaySeconds: 180
    timeoutSeconds: 1
    periodSeconds: 60
    failureThreshold: 3
    successThreshold: 1
    httpGet:
      path: /metrics
      port: 5064
      scheme: HTTP
  livenessProbe:
    initialDelaySeconds: 60
    timeoutSeconds: 1
    periodSeconds: 60
    failureThreshold: 60
    successThreshold: 1
    httpGet:
      path: /metrics
      port: 5064
      scheme: HTTP

  # If false, data ownership will not be reset at startup
  # This allows the beacon node to be run with an arbitrary user
  initChownData:
    enabled: true
    # initChownData container image
    image:
      repository: "busybox"
      tag: "1.33"
      pullPolicy: IfNotPresent

  # Whether or not to allocate persistent volume disk for the data directory.
  # In case of pod failure, the pod data directory will still persist.
  persistence:
    enabled: true
    storageClassName: ""
    accessModes:
      - ReadWriteOnce
    size: 10Gi

  # Affinity Settings
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  affinity: { }

  # Toleration Settings
  # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations: [ ]

  # used to assign priority to pods
  # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  priorityClassName: ""