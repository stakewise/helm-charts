# Default values for prysm.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Pod Security Context
# ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
securityContext:
  fsGroup: 1001
  runAsUser: 1001

# A node which implements the DiscoveryV5 protocol for peer
# discovery. The purpose of this service is to provide a starting point for
# newly connected services to find other peers outside of their network.
bootnode:
  # Whether to deploy bootnode or not.
  enabled: false

  # Service Cluster IP
  clusterIP: ""

  # Docker image
  image:
    repository: "gcr.io/prysmaticlabs/prysm/bootnode"
    tag: "HEAD-4f31ba"
    pullPolicy: IfNotPresent

  # Optional private key used for identity
  privateKey: ""

  # Extra flags to pass to the bootnode
  extraFlags:
    - "--external-ip=$(POD_IP)"

  # Configure resource requests and limits.
  # http://kubernetes.io/docs/user-guide/compute-resources/
  resources:
    requests:
      cpu: 2m
      memory: 25Mi

  # used to assign priority to pods
  # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  priorityClassName: "high-priority"


# Beacon chain is responsible for running a full Proof-of-Stake blockchain
beacon:
  # Whether to deploy beacon chain or not.
  enabled: true

  # How many beacon chain pods to run simultaneously
  replicas: 2

  # Docker image
  image:
    repository: "gcr.io/prysmaticlabs/prysm/beacon-chain"
    #tag: "v1.4.4"
    tag: "v2.0.0-rc.1"
    pullPolicy: IfNotPresent

  # Enable pod disruption budget
  # https://kubernetes.io/docs/tasks/run-application/configure-pdb
  podDisruptionBudget:
    enabled: true
    maxUnavailable: 1

  # Vertical Pod Autoscaler config
  # Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler
  verticalAutoscaler:
    # If true a VPA object will be created for the StatefulSet
    enabled: false
    updateMode: Off
    containerPolicies: { }

  extraFlags:
    # Beacon chain options
    - "--accept-terms-of-use"
    - "--http-web3provider=<eth1 chain endpoint>"

    # p2p options
    - "--p2p-max-peers=100"
    - "--enable-peer-scorer"

  metrics:
    # Whether to enable metrics collection or not
    enabled: false

    port: 9090

    # Extra flags to pass for collecting metrics
    flags:
      - "--monitoring-port=9090"
      - "--monitoring-host=0.0.0.0"

    # Metrics collection annotations to add to service
    svcAnnotations:
      prometheus.io/scrape: "true"
    
    ## Prometheus Service Monitor
    ## ref: https://github.com/coreos/prometheus-operator
    ##      https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor resource(s) for scraping metrics using PrometheusOperator
      ##
      enabled: false
      ## @param metrics.serviceMonitor.namespace The namespace in which the ServiceMonitor will be created
      ##
      namespace: ""
      ## @param metrics.serviceMonitor.interval The interval at which metrics should be scraped
      ##
      interval: 30s
      ## @param metrics.serviceMonitor.scrapeTimeout The timeout after which the scrape is ended
      ##
      scrapeTimeout: ""
      ## @param metrics.serviceMonitor.relabellings Metrics RelabelConfigs to apply to samples before scraping.
      ##
      relabellings: []
      ## @param metrics.serviceMonitor.metricRelabelings Metrics RelabelConfigs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param metrics.serviceMonitor.additionalLabels Additional labels that can be used so ServiceMonitor resource(s) can be discovered by Prometheus
      ##
      additionalLabels: {}
    ## Custom PrometheusRule to be defined
    ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions
    ##
    prometheusRule:
      ## @param metrics.prometheusRule.enabled Create a custom prometheusRule Resource for scraping metrics using PrometheusOperator
      ##
      enabled: false
      ## @param metrics.prometheusRule.namespace The namespace in which the prometheusRule will be created
      ##
      namespace: ""
      ## @param metrics.prometheusRule.additionalLabels Additional labels for the prometheusRule
      ##
      additionalLabels: {}
      ## @param metrics.prometheusRule.rules Custom Prometheus rules
      ## e.g:
      ## rules:
      ##   - alert: GethNodeDown
      ##     expr: up{job="{{ include "openethereum.fullname" . }}-node"} == 0
      ##     for: 5m
      ##     labels:
      ##       severity: error
      ##     annotations:
      ##       summary: Geth Node {{ "{{ $labels.instance }}" }} down
      ##       description: Geth Node {{ "{{ $labels.instance }}" }} is down
      ##
      rules: []

  # defines whether the service must be headless
  svcHeadless: false

  # Configure session affinity for validator clients to hit the same beacon node
  # for the period specified in `timeoutSeconds`
  # https://kubernetes.io/docs/concepts/services-networking/service/#proxy-mode-userspace
  sessionAffinity:
    # Whether to enable session affinity or not
    enabled: false
    # The session duration in seconds
    timeoutSeconds: 86400

  # Configure resource requests and limits.
  # http://kubernetes.io/docs/user-guide/compute-resources/
  resources: { }

  # Configure liveness and readiness probes
  # https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # NB! readinessProbe and livenessProbe must be disabled before genesis
  readinessProbe:
    initialDelaySeconds: 180
    timeoutSeconds: 1
    periodSeconds: 60
    failureThreshold: 3
    successThreshold: 1
    httpGet:
      path: /healthz
      port: 9090
      scheme: HTTP
  livenessProbe:
    initialDelaySeconds: 60
    timeoutSeconds: 1
    periodSeconds: 60
    failureThreshold: 60
    successThreshold: 1
    httpGet:
      path: /healthz
      port: 9090
      scheme: HTTP

  # If false, data ownership will not be reset at startup
  # This allows the beacon node to be run with an arbitrary user
  initChownData:
    enabled: true
    # initChownData container image
    image:
      repository: "busybox"
      tag: "1.33"
      pullPolicy: IfNotPresent

  # Whether or not to allocate persistent volume disk for the data directory.
  # In case of pod failure, the pod data directory will still persist.
  persistence:
    enabled: true
    storageClassName: ""
    accessModes:
      - ReadWriteOnce
    size: 350Gi

  # Affinity Settings
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  affinity: { }

  # Toleration Settings
  # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations: [ ]

  # used to assign priority to pods
  # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  priorityClassName: ""

# Validators is responsible for performing block proposals and attestations
validator:
  # Whether to deploy beacon chain or not.
  enabled: false

  # How many beacon chain pods to run simultaneously
  replicas: 1

  # Docker image
  image:
    repository: "gcr.io/prysmaticlabs/prysm/validator"
    #tag: "v1.4.4"
    tag: "v2.0.0-rc.2"
    pullPolicy: IfNotPresent

  # Enable pod disruption budget
  # https://kubernetes.io/docs/tasks/run-application/configure-pdb
  podDisruptionBudget:
    enabled: true
    maxUnavailable: 1

  # Vertical Pod Autoscaler config
  # Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler
  verticalAutoscaler:
    # If true a VPA object will be created for the StatefulSet
    enabled: false
    updateMode: Off
    containerPolicies: { }

  extraFlags:
    - "--accept-terms-of-use"
    - "--graffiti=StakeWise"
    - "--disable-rewards-penalties-logging"
    - "--disable-account-metrics"
    - "--attest-timely"
    - "--enable-doppelganger"

  metrics:
    # Whether to enable metrics collection or not
    enabled: false

    port: 9090

    # Extra flags to pass for collecting metrics
    flags:
      - "--monitoring-port=9090"
      - "--monitoring-host=0.0.0.0"

    # Metrics collection annotations to add to service
    svcAnnotations:
      prometheus.io/scrape: "true"
    
    ## Prometheus Service Monitor
    ## ref: https://github.com/coreos/prometheus-operator
    ##      https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor resource(s) for scraping metrics using PrometheusOperator
      ##
      enabled: false
      ## @param metrics.serviceMonitor.namespace The namespace in which the ServiceMonitor will be created
      ##
      namespace: ""
      ## @param metrics.serviceMonitor.interval The interval at which metrics should be scraped
      ##
      interval: 30s
      ## @param metrics.serviceMonitor.scrapeTimeout The timeout after which the scrape is ended
      ##
      scrapeTimeout: ""
      ## @param metrics.serviceMonitor.relabellings Metrics RelabelConfigs to apply to samples before scraping.
      ##
      relabellings: []
      ## @param metrics.serviceMonitor.metricRelabelings Metrics RelabelConfigs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param metrics.serviceMonitor.additionalLabels Additional labels that can be used so ServiceMonitor resource(s) can be discovered by Prometheus
      ##
      additionalLabels: {}
    ## Custom PrometheusRule to be defined
    ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions
    ##
    prometheusRule:
      ## @param metrics.prometheusRule.enabled Create a custom prometheusRule Resource for scraping metrics using PrometheusOperator
      ##
      enabled: false
      ## @param metrics.prometheusRule.namespace The namespace in which the prometheusRule will be created
      ##
      namespace: ""
      ## @param metrics.prometheusRule.additionalLabels Additional labels for the prometheusRule
      ##
      additionalLabels: {}
      ## @param metrics.prometheusRule.rules Custom Prometheus rules
      ## e.g:
      ## rules:
      ##   - alert: GethNodeDown
      ##     expr: up{job="{{ include "openethereum.fullname" . }}-node"} == 0
      ##     for: 5m
      ##     labels:
      ##       severity: error
      ##     annotations:
      ##       summary: Geth Node {{ "{{ $labels.instance }}" }} down
      ##       description: Geth Node {{ "{{ $labels.instance }}" }} is down
      ##
      rules: []

  # Configure resource requests and limits.
  # http://kubernetes.io/docs/user-guide/compute-resources/
  resources: { }

  # Configure liveness and readiness probes
  # https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # NB! readinessProbe and livenessProbe must be disabled before genesis
  readinessProbe:
    initialDelaySeconds: 180
    timeoutSeconds: 1
    periodSeconds: 60
    failureThreshold: 3
    successThreshold: 1
    httpGet:
      path: /healthz
      port: 9090
      scheme: HTTP
  livenessProbe:
    initialDelaySeconds: 60
    timeoutSeconds: 1
    periodSeconds: 60
    failureThreshold: 60
    successThreshold: 1
    httpGet:
      path: /healthz
      port: 9090
      scheme: HTTP

  # If false, data ownership will not be reset at startup
  # This allows the beacon node to be run with an arbitrary user
  initChownData:
    enabled: true
    # initChownData container image
    image:
      repository: "busybox"
      tag: "1.33"
      pullPolicy: IfNotPresent

  # Whether or not to allocate persistent volume disk for the data directory.
  # In case of pod failure, the pod data directory will still persist.
  persistence:
    enabled: true
    storageClassName: ""
    accessModes:
      - ReadWriteOnce
    size: 10Gi

  # Affinity Settings
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  affinity: { }

  # Toleration Settings
  # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations: [ ]

  # used to assign priority to pods
  # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  priorityClassName: ""