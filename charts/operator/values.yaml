# Default values for operator.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

## Provide a name in place of operator for `app:` labels
##
nameOverride: ""

## Provide a name to substitute for the full names of resources
##
fullnameOverride: ""

## Pod Security Context
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
##
securityContext:
  fsGroup: 1001
  runAsUser: 1001

## RBAC configuration.
## ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/
##
rbac:
  create: true

## Init image is used to chown data volume, initialise genesis, etc.
##
initImageBusybox:
  repository: "busybox"
  tag: "1.34"
  pullPolicy: IfNotPresent


## Configuration for validators
## ref: https://lighthouse-book.sigmaprime.io/validator-management.html
## ref: https://docs.prylabs.network/docs/getting-started
##

## Whether to enable validator statefulset or not
## Can be used to temporarily disable validators 
## until synchronization of eth1/eth2 nodes is complete
##
enabled: false

## What type of validator to use.
## Options: prysm, lighthouse
##
type: prysm

## How many validators to run
## **NB! Every validators hosts up to 100 keys, so the number of validators must be >= (number of validator keys) / 100
##
validatorsCount: 1

## Validators image version
## ref: https://gcr.io/prysmaticlabs/prysm/validator
## ref: https://hub.docker.com/r/sigp/lighthouse
image:
  pullPolicy: IfNotPresent
  prysm:
    repository: "gcr.io/prysmaticlabs/prysm/validator"
    tag: "v2.0.3"
  lighthouse:
    repository: "sigp/lighthouse"
    tag: "v2.0.1"
  
## Init image is used to initialise validators
##
initImage:
  repository: "unxnn/validator-init"
  tag: "stable"
  pullPolicy: IfNotPresent
  
terminationGracePeriodSeconds: 900

## Spearate service account per validator.
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
##
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  
## Node labels for pod assignment
## ref: https://kubernetes.io/docs/user-guide/node-selection/
##
nodeSelector: {}

## Tolerations for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
##
tolerations: []

## Affinity for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
##
affinity: {}

## used to assign priority to pods
## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
##
priorityClassName: ""

## Vertical Pod Autoscaler config
## ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler
##
verticalAutoscaler:
  # If true a VPA object will be created for the StatefulSet
  enabled: false
  updateMode: Off
  containerPolicies: { }
  
## Network ID
## Options: mainnet, prater
##
networkID: "mainnet"

## Validators flags
##
flags:
  prysm:
    - "--datadir=/data/prysm"
    - "--wallet-dir=/data/prysm/validators"
    - "--wallet-password-file=/data/prysm/validators/password.txt"
    - "--accept-terms-of-use"
    - "--disable-rewards-penalties-logging"
    - "--disable-account-metrics"
    - "--enable-doppelganger"
  lighthouse:
    - "lighthouse"
    - "vc"
    - "--datadir=/data/lighthouse"
  
## Validators extra flags
##
extraFlags:
  prysm: []
  lighthouse: []

## Beacon Chain node address
## If not specified, will connect to prysm/lighthouse beacon nodes
## deployed with this helm chart
beaconChainRpcEndpoint: ""

## You can use the graffiti to add a string to your proposed blocks,
## which will be seen on the block explorer.
## ref: https://docs.prylabs.network/docs/prysm-usage/parameters#validator-configuration
##
graffiti: ""

## Monitoring
##
metrics:
  # Whether to enable metrics collection or not
  enabled: false

  # Prometheus exporter port
  port: 9090

  # Extra flags to pass for collecting metrics
  flags:
    prysm:
      - "--monitoring-port=9090"
      - "--monitoring-host=0.0.0.0"
    lighthouse:
      - "--metrics"
      - "--metrics-port=9090"
      - "--metrics-address=0.0.0.0"
    
  ## Prometheus Service Monitor
  ## ref: https://github.com/coreos/prometheus-operator
  ##      https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
  ##
  serviceMonitor:
    ## @param metrics.serviceMonitor.enabled Create ServiceMonitor resource(s) for scraping metrics using PrometheusOperator
    ##
    enabled: false
    ## @param metrics.serviceMonitor.namespace The namespace in which the ServiceMonitor will be created
    ##
    namespace: ""
    ## @param metrics.serviceMonitor.interval The interval at which metrics should be scraped
    ##
    interval: 30s
    ## @param metrics.serviceMonitor.scrapeTimeout The timeout after which the scrape is ended
    ##
    scrapeTimeout: ""
    ## @param metrics.serviceMonitor.relabellings Metrics RelabelConfigs to apply to samples before scraping.
    ##
    relabellings: []
    ## @param metrics.serviceMonitor.metricRelabelings Metrics RelabelConfigs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
    ##
    honorLabels: false
    ## @param metrics.serviceMonitor.additionalLabels Additional labels that can be used so ServiceMonitor resource(s) can be discovered by Prometheus
    ##
    additionalLabels: {}
  ## Custom PrometheusRule to be defined
  ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions
  ##
  prometheusRule:
    ## @param metrics.prometheusRule.enabled Create a custom prometheusRule Resource for scraping metrics using PrometheusOperator
    ##
    enabled: false
    ## @param metrics.prometheusRule.namespace The namespace in which the prometheusRule will be created
    ##
    namespace: ""
    ## @param metrics.prometheusRule.additionalLabels Additional labels for the prometheusRule
    ##
    additionalLabels: {}
    ## @param metrics.prometheusRule.rules Custom Prometheus rules
    ## e.g:
    ## rules:
    ##   prysm:
    ##     - alert: PrysmValidatorHourlyEarningLessOrEqual0
    ##       expr: sum(validator_balance) - sum(validator_balance offset 1h) - count(validator_balance > 16)*32 + count(validator_balance offset 1h > 0)*32
    ##       for: 5m
    ##       labels:
    ##         severity: critical
    ##       annotations:
    ##         summary: Prysm validator hourly earning <= 0
    ##         description: Check validators immediately. Pod - {{ printf "{{ $labels.pod }}" }}. Namespace - {{ printf "{{ $labels.namespace }}" }}
    ##     - alert: PrysmValidatorAlotOfErrorsLastHour
    ##       expr: sum(delta(log_entries_total{job='{{ include "operator.fullname" . }}-validator', level="error"}[1h]) > 0) 
    ##       for: 5m
    ##       labels:
    ##         severity: warning
    ##       annotations:
    ##         summary: Many validator errors or warnings last hour
    ##         description: Check validator {{ printf "{{ $labels.pod }}" }}. Namespace - {{ printf "{{ $labels.namespace }}" }}
    ##   lighthouse: []
    ##
    rules: []

## Configure resource requests and limits.
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
##
resources: {}

## Configure liveness and readiness probes
## https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
##
readinessProbe:
  prysm:
    initialDelaySeconds: 60
    timeoutSeconds: 1
    periodSeconds: 60
    failureThreshold: 3
    successThreshold: 1
    httpGet:
      path: /healthz
      port: metrics
      scheme: HTTP
  lighthouse:
    initialDelaySeconds: 60
    timeoutSeconds: 1
    periodSeconds: 60
    failureThreshold: 3
    successThreshold: 1
    httpGet:
      path: /metrics
      port: metrics
      scheme: HTTP
livenessProbe:
  prysm:
    initialDelaySeconds: 60
    timeoutSeconds: 1
    periodSeconds: 60
    failureThreshold: 3
    successThreshold: 1
    httpGet:
      path: /healthz
      port: metrics
      scheme: HTTP
  lighthouse:
    initialDelaySeconds: 60
    timeoutSeconds: 1
    periodSeconds: 60
    failureThreshold: 3
    successThreshold: 1
    httpGet:
      path: /metrics
      port: metrics
      scheme: HTTP

## If false, data ownership will not be reset at startup
## This allows the geth node to be run with an arbitrary user
##
initChownData: true

## Whether or not to allocate persistent volume disk for the data directory.
## In case of pod failure, the pod data directory will still persist.
##
persistence:
  enabled: true
  storageClassName: ""
  accessModes:
    - ReadWriteOnce
  size: 10Gi


## Configuration for HashiCorp Vault
## ref: https://www.vaultproject.io/docs/platform/k8s/helm
##

## Existing vault address
## Used when vault.enabled=false
##
vaultAddr: ""

vault:
  enabled: false
  injector:
    enabled: false
  server:
    enabled: true
    ha:
      enabled: true
      replicas: 3
      raft:
        enabled: true
        # Note: Configuration files are stored in ConfigMaps so sensitive data
        # such as passwords should be either mounted through extraSecretEnvironmentVars
        # or through a Kube secret.  For more information see:
        # https://www.vaultproject.io/docs/platform/k8s/helm/run#protecting-sensitive-vault-configurations
        config: |
          ui = true
          listener "tcp" {
            tls_disable = 1
            address = "[::]:8200"
            cluster_address = "[::]:8201"
          }
          storage "raft" {
            path = "/vault/data"
          }
          service_registration "kubernetes" {}
          # Example configuration for using auto-unseal, using Google Cloud KMS. The
          # GKMS keys must already exist, and the cluster must have a service account
          # that is authorized to access GCP KMS.
          #seal "gcpckms" {
          #   project     = "vault-helm-dev-246514"
          #   region      = "global"
          #   key_ring    = "vault-helm-unseal-kr"
          #   crypto_key  = "vault-helm-unseal-key"
          #}
    # extraEnvironmentVars is a list of extra environment variables to set with the stateful set. These could be
    # used to include variables required for auto-unseal.
    extraEnvironmentVars: {}
      # GOOGLE_REGION: global
      # GOOGLE_PROJECT: myproject
      # GOOGLE_APPLICATION_CREDENTIALS: /vault/userconfig/myproject/myproject-creds.json

    # volumes is a list of volumes made available to all containers. These are rendered
    # via toYaml rather than pre-processed like the extraVolumes value.
    # The purpose is to make it easy to share volumes between containers.
    volumes: null
    #   - name: plugins
    #     emptyDir: {}

    # volumeMounts is a list of volumeMounts for the main server container. These are rendered
    # via toYaml rather than pre-processed like the extraVolumes value.
    # The purpose is to make it easy to share volumes between containers.
    volumeMounts: null
    #   - mountPath: /usr/local/libexec/vault
    #     name: plugins
    #     readOnly: true
  ui:
    enabled: true

## Geth node configuration
##
geth:
  enabled: true
  replicas: 2

## Erigon node configuration
##
## Erigon can be used as an execution-layer for beacon chain consensus clients (Eth2).
## Default configuration is ok.
## Once the JSON-RPC daemon is running, all you need to do is point your
## beacon chain client to erigon:8545, where is either localhost
## or the IP address of the device running the JSON-RPC daemon.
## Erigon has been tested with Lighthouse however all other clients that
## support JSON-RPC should also work.
erigon:
  enabled: false
  replicas: 1
  chain: mainnet

## Prysm beacon chain node configuration
##
prysm:
  enabled: true
  replicas: 3
  eth1Endpoint: "http://operator-geth:8545"

## Lighthouse beacon chain node configuration
##
lighthouse:
  enabled: true
  replicas: 1
  networkID: mainnet
  eth1Endpoints:
    - http://operator-geth:8545
